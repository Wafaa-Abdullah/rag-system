{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49200b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q sentence-transformers faiss-cpu datasets transformers fastapi uvicorn pyngrok torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ea76f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "import subprocess, time\n",
    "\n",
    "ollama_process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "!ollama pull qwen2.5:0.5b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1060f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    DATASET_SIZE = 1000\n",
    "    CHUNK_SIZE = 400\n",
    "    CHUNK_OVERLAP = 50\n",
    "    TOP_K_RETRIEVAL = 3\n",
    "\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "    OLLAMA_MODEL = \"qwen2.5:0.5b\"\n",
    "    OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "    DATA_DIR = \"/content/data\"\n",
    "    FAISS_INDEX_PATH = \"/content/data/faiss.index\"\n",
    "    CHUNKS_PATH = \"/content/data/chunks.json\"\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.DATA_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dfa5267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading embedding model...\n",
      "ðŸ“š Loading TriviaQA dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9084bb0bfef466796db8e2b83cff5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Chunking documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6831893c6a44f45b734e4a3c659cd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total chunks: 1000\n",
      "ðŸ”¢ Creating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51954eb57eaf4d79850d89959c41c54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Building FAISS index...\n",
      "âœ… FAISS index size: 1000\n"
     ]
    }
   ],
   "source": [
    "import faiss, numpy as np\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        print(\"Loading embedding model...\")\n",
    "        self.embedding_model = SentenceTransformer(config.EMBEDDING_MODEL)\n",
    "        self.chunks = []\n",
    "        self.index = None\n",
    "\n",
    "    def load_and_process(self):\n",
    "        print(\"Loading TriviaQA dataset...\")\n",
    "        dataset = load_dataset(\"trivia_qa\", \"unfiltered.nocontext\", split=\"train\")\n",
    "        dataset = dataset.select(range(config.DATASET_SIZE))\n",
    "\n",
    "        print(\"Chunking documents...\")\n",
    "        for item in tqdm(dataset):\n",
    "            text = f\"Question: {item['question']} Answer: {item['answer']['value']}\"\n",
    "            words = text.split()\n",
    "\n",
    "            chunk_size = config.CHUNK_SIZE // 4\n",
    "            overlap = config.CHUNK_OVERLAP // 4\n",
    "\n",
    "            for i in range(0, len(words), chunk_size - overlap):\n",
    "                chunk = \" \".join(words[i:i + chunk_size])\n",
    "                if len(chunk) > 30:\n",
    "                    self.chunks.append({\"text\": chunk})\n",
    "\n",
    "        print(f\"Total chunks: {len(self.chunks)}\")\n",
    "\n",
    "    def build_faiss_index(self):\n",
    "        print(\"Creating embeddings...\")\n",
    "        texts = [c[\"text\"] for c in self.chunks]\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            texts,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        print(\"Building FAISS index...\")\n",
    "        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "        print(f\"FAISS index size: {self.index.ntotal}\")\n",
    "processor = DocumentProcessor()\n",
    "processor.load_and_process()\n",
    "processor.build_faiss_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09e8fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self, chunks, index):\n",
    "        self.chunks = chunks\n",
    "        self.index = index\n",
    "        self.embedder = SentenceTransformer(config.EMBEDDING_MODEL)\n",
    "\n",
    "    def retrieve(self, query, k):\n",
    "        q_emb = self.embedder.encode([query]).astype(\"float32\")\n",
    "        dists, ids = self.index.search(q_emb, k)\n",
    "\n",
    "        contexts = [self.chunks[i][\"text\"] for i in ids[0]]\n",
    "        scores = [1 / (1 + d) for d in dists[0]]\n",
    "\n",
    "        return contexts, scores\n",
    "\n",
    "retriever = Retriever(processor.chunks, processor.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce25442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMHandler:\n",
    "    def generate(self, question, contexts):\n",
    "        prompt = f\"\"\"\n",
    "You are a question-answering system.\n",
    "\n",
    "Answer the question using ONLY the information in the context below.\n",
    "If the answer is NOT present in the context, say exactly:\n",
    "\"I don't know based on the provided context.\"\n",
    "\n",
    "Context:\n",
    "{contexts}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        r = requests.post(\n",
    "            f\"{config.OLLAMA_URL}/api/generate\",\n",
    "            json={\n",
    "                \"model\": config.OLLAMA_MODEL,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return r.json()[\"response\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1e3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     154.183.132.69:0 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     154.183.132.69:0 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     154.183.132.69:0 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     154.183.132.69:0 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     154.183.132.69:0 - \"POST /query HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "class RAGPipeline:\n",
    "    def query(self, question, top_k):\n",
    "        start = time.time()\n",
    "\n",
    "        contexts, scores = retriever.retrieve(question, top_k)\n",
    "\n",
    "        latency = int((time.time() - start) * 1000)\n",
    "\n",
    "        # Similarity threshold\n",
    "        if len(scores) == 0 or max(scores) < 0.6:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": \"I don't know based on the provided context.\",\n",
    "                \"contexts\": contexts,\n",
    "                \"scores\": scores,\n",
    "                \"latency_ms\": latency\n",
    "            }\n",
    "\n",
    "        answer = llm.generate(question, \"\\n\".join(contexts))\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"contexts\": contexts,\n",
    "            \"scores\": scores,\n",
    "            \"latency_ms\": latency\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08d09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "app = FastAPI(title=\"RAG TriviaQA API\")\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "    top_k: int = 3\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "    contexts: List[str]\n",
    "    scores: List[float]\n",
    "    latency_ms: int\n",
    "\n",
    "@app.get(\"/\")\n",
    "def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "@app.post(\"/query\", response_model=QueryResponse)\n",
    "def query_api(req: QueryRequest):\n",
    "    return rag_pipeline.query(req.question, req.top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2b7433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pyngrok.process.ngrok:t=2025-12-26T13:14:58+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Public URL: NgrokTunnel: \"https://factious-erlene-postethmoid.ngrok-free.dev\" -> \"http://localhost:8000\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [27841]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     154.183.132.69:0 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     154.183.132.69:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     154.183.132.69:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     154.183.132.69:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     154.183.132.69:0 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     154.183.132.69:0 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     154.183.132.69:0 - \"POST /query HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "import uvicorn, threading\n",
    "\n",
    "ngrok.set_auth_token(\"?????\")\n",
    "\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"Public URL:\", public_url)\n",
    "\n",
    "def run():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "threading.Thread(target=run).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85cae086",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"What is the capital of Kenya?\", \"answer\": \"Nairobi\"},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare\"},\n",
    "    {\"question\": \"Who painted the Mona Lisa?\", \"answer\": \"Leonardo da Vinci\"},\n",
    "    {\"question\": \"What is the largest planet in our solar system?\", \"answer\": \"Jupiter\"},\n",
    "    {\"question\": \"Who was the first President of the United States?\", \"answer\": \"George Washington\"},\n",
    "    {\"question\": \"What year did World War II end?\", \"answer\": \"1945\"},\n",
    "    {\"question\": \"Who invented the telephone?\", \"answer\": \"Alexander Graham Bell\"},\n",
    "    {\"question\": \"What is the smallest country in the world?\", \"answer\": \"Vatican City\"},\n",
    "    {\"question\": \"What is the chemical symbol for gold?\", \"answer\": \"Au\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c934b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_rag(sample_questions, rag_pipeline):\n",
    "    results = []\n",
    "\n",
    "    for q in sample_questions:\n",
    "        res = rag_pipeline.query(q[\"question\"], top_k=3)\n",
    "        \n",
    "        # Check if any retrieved context contains the answer\n",
    "        retrieved_correct = any(q[\"answer\"].lower() in ctx.lower() for ctx in res[\"contexts\"])\n",
    "        \n",
    "        # Determine answer correctness\n",
    "        if res[\"answer\"].lower() == q[\"answer\"].lower():\n",
    "            correctness = \"Correct\"\n",
    "        elif q[\"answer\"].lower() in res[\"answer\"].lower():\n",
    "            correctness = \"Partially Correct\"\n",
    "        else:\n",
    "            correctness = \"Incorrect\"\n",
    "        \n",
    "        results.append({\n",
    "            \"Question\": q[\"question\"],\n",
    "            \"Retrieved Context Correct?\": \"Yes\" if retrieved_correct else \"No\",\n",
    "            \"Generated Answer\": res[\"answer\"],\n",
    "            \"Answer Correctness\": correctness,\n",
    "            \"Latency (ms)\": res[\"latency_ms\"]\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Summary\n",
    "    total = len(df)\n",
    "    retrieved_yes = df[\"Retrieved Context Correct?\"].value_counts().get(\"Yes\", 0)\n",
    "    correct_ans = df[\"Answer Correctness\"].value_counts().get(\"Correct\", 0)\n",
    "    partially_correct = df[\"Answer Correctness\"].value_counts().get(\"Partially Correct\", 0)\n",
    "    avg_latency = df[\"Latency (ms)\"].mean()\n",
    "    \n",
    "    print(\"\\nEvaluation Summary\")\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Retrieved Context Correct: {retrieved_yes}/{total} ({retrieved_yes/total*100:.1f}%)\")\n",
    "    print(f\"Answers Correct: {correct_ans}/{total} ({correct_ans/total*100:.1f}%)\")\n",
    "    print(f\"Partially Correct Answers: {partially_correct}/{total} ({partially_correct/total*100:.1f}%)\")\n",
    "    print(f\"Average Latency: {avg_latency:.0f} ms\\n\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1825c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Evaluation Summary\n",
      "Total Questions: 10\n",
      "Retrieved Context Correct: 2/10 (20.0%)\n",
      "Answers Correct: 0/10 (0.0%)\n",
      "Partially Correct Answers: 6/10 (60.0%)\n",
      "Average Latency: 10887 ms\n",
      "\n",
      "                                            Question  \\\n",
      "0                     What is the capital of France?   \n",
      "1                      What is the capital of Kenya?   \n",
      "2                        Who wrote Romeo and Juliet?   \n",
      "3                         Who painted the Mona Lisa?   \n",
      "4    What is the largest planet in our solar system?   \n",
      "5  Who was the first President of the United States?   \n",
      "6                    What year did World War II end?   \n",
      "7                        Who invented the telephone?   \n",
      "8         What is the smallest country in the world?   \n",
      "9              What is the chemical symbol for gold?   \n",
      "\n",
      "  Retrieved Context Correct?  \\\n",
      "0                         No   \n",
      "1                        Yes   \n",
      "2                         No   \n",
      "3                        Yes   \n",
      "4                         No   \n",
      "5                         No   \n",
      "6                         No   \n",
      "7                         No   \n",
      "8                         No   \n",
      "9                         No   \n",
      "\n",
      "                                    Generated Answer Answer Correctness  \\\n",
      "0  To answer the questions, I'll use the context ...  Partially Correct   \n",
      "1                   The capital of Kenya is Nairobi.  Partially Correct   \n",
      "2  The answer is Pythagoras. He lived in ancient ...          Incorrect   \n",
      "3  The actual title of Leonardo da Vinci's \"Mona ...  Partially Correct   \n",
      "4  The answer is Saturn. It is known for having m...          Incorrect   \n",
      "5  The first president of the United States was G...  Partially Correct   \n",
      "6                        World War II ended in 1945.  Partially Correct   \n",
      "7  According to the given context, Herman Holleri...          Incorrect   \n",
      "8  The answer to your questions is as follows:\\n\\...          Incorrect   \n",
      "9                The chemical symbol for gold is Au.  Partially Correct   \n",
      "\n",
      "   Latency (ms)  \n",
      "0         21689  \n",
      "1          3993  \n",
      "2          4534  \n",
      "3         16198  \n",
      "4          5178  \n",
      "5          6338  \n",
      "6          3556  \n",
      "7         26654  \n",
      "8         16849  \n",
      "9          3885  \n"
     ]
    }
   ],
   "source": [
    "evaluation_df = evaluate_rag(sample_questions, rag_pipeline)\n",
    "print(evaluation_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
